---
title: Insert title here
key: 07185d5d9a25552df760cdb9f65fef3f

---
## Maximum Likelihood Estimate (MLE)

```yaml
type: "TitleSlide"
key: "62a66e100f"
```

`@lower_third`

name: Akshay Sapra
title: Data Scientist


`@script`
Hello and Welcome to the course. In this video you will be learning about Maximum Likelihood Estimate.


---
## What is Linear Regression ?

```yaml
type: "FullSlide"
key: "ee62362bc8"
```

`@part1`
A method to predict numerical variables from the set of other variables.

Categorized under Supervised Learning.
- Values to be predicted are called target, denoted by `(y1, y2, y3.. yn)` {{2}}

- Variables used for prediction are called predictors, denoted by `(x1, x2.. xn)` {{2}}

Multiple Linear Regression is denoted by below equation{{3}}

![](https://assets.datacamp.com/production/repositories/4490/datasets/9f6d561469b203aae10e6c3e1db73de7fce2fd3b/MLR.JPG){{3}}


`@script`
Linear Regression is the most basic model of predicting real/numerical values.
As this model learns to predict value under the supervision or with the help of other variables we categorize it under supervised learning. 
Variables that are to be predicted are denoted by y and called target variables.
And the variables that are used to predict are called predictors often denoted by X.
Multiple linear regression is of the form Y=B0X0+B1X1+B2X2+BnXn.


---
## Before we Start,

```yaml
type: "FullSlide"
key: "7f9f7d9dee"
```

`@part1`
Concept of distribution of Population and their parameters.
For Example,
- If the population is normally distributed, it's parameters are it's mean and standard deviation.
- If the population is Bernoulli distribution then it's parameter will be Probability of Success (P).


`@script`
I hope you already are familiar with populations and their parameters. 
The parameters are numerical values associated with those populations. 
For example, if you have a normal population, you can characterise everything about the population using its mean and standard deviation.
Or for a Bernoulli population, the Probability of Success P will tell everything about that population.


---
## What is this Estimate ?

```yaml
type: "FullSlide"
key: "b102e0e3a8"
```

`@part1`
Any *Statistic* used to estimate the value of an unknown parameter is called **estimator** and the *observed value* of the estimator is called **estimate**.

 **Example:**   Obtain MLE for the *mean* number of customers coming to the restaurant in a day. Assuming customer comes at a constant rate in a day.{{2}}

`We take only 10 samples, where the number of customers comes in each sample are:  5, 6, 10, 9, 8, 7, 8, 7, 10, 11`{{2}}


`@script`
To know about MLE let us brief about Estimate. 
Any Statistic used to estimate the value of an unknown parameter (which are associated with the population) is called estimator and the observed value of the estimator is called estimate.
To understand it practically, let us see an example.
Consider yourself working in a restaurant and you want to estimate the average customers that will come to the restaurant each day. 
Data we have is the number of customers coming each day. 
In this scenario, we can use MLE to find the average of the whole population by going through the sample. 
And as we have assumed that customers are coming at a constant rate, therefore we will use Poisson distribution, If you are unsure of the reason I recommend going through the Distribution lessons.


---
## Maximum Likelihood Estimator (MLE)

```yaml
type: "FullSlide"
key: "50c9ffcefb"
```

`@part1`
It is a method of estimating the parameters of a population by **maximising** the **likelihood function**.


     data <- c(5, 6, 10, 9, 8, 7, 8, 7, 10, 11)

    #computational function 
    fn <- function(lambda) { 
       val <- 0 
       for (i in 1:10) { 
           # OK for skipping factorial(data[i])
           val <- val + data[i]*log(lambda) - lambda
       } 
       return(val) 
    } 

    optimize(fn, c(2,15), maximum = TRUE, tol = .Machine$double.eps^0.25)
{{2}}


`@script`
We will solve the example by the method of MLE. It is a method of estimating the parameters of a population by maximising the likelihood function.

to solve the example in R, we will 1st import the data.

and then create a function to do the computation.
Instead of calculating it directly, we have used the expanded expression to calculate the maximum likelihood of poisson.

We have used optimise function to find the optimal parameters for the given dataset.


---
## Explanation

```yaml
type: "FullSlide"
key: "83c71c92ca"
```

`@part1`
![](https://assets.datacamp.com/production/repositories/4490/datasets/3982908710e3ba1bfcc95338f2e9c16838e83d34/MLE%20Slide.JPG)


`@script`
to understand MLE mathematically, we can assume x1 ,x2 are continous random variable and their probability density function is donted by function f of Thetha where Thetha is assumed to be unknown.

Now Since F of Thetha represents the likelihood that values x1x2 will be observed when Thetha is the true value of the parameter.

 In other words, the maximum likelihood estimate Thetha Cap is deﬁned to be that value of θ maximizing f (x1,...,xn|θ) where x1,...,xn are the observed values. The function f (x1,...,xn|θ) is often referred to as the likelihood function of θ.

In determining the maximizing value of θ, it is often useful to use the fact that f (x1,...,xn|θ) and log [f (x1,...,xn|θ)] have their maximum at the same value of θ. Hence, we may also obtain  Thetha Cap by maximizing log[f (x1,...,xn|θ)].


---
## MLE of some commomn Distributions

```yaml
type: "TwoRowsTwoColumns"
key: "169ff1b5e0"
```

`@part1`
a). MLE of Normal Parameters. 

![](https://assets.datacamp.com/production/repositories/4490/datasets/2b2a60db6dcd226dc340c21c765a6efd259bc7c8/MLE%20of%20Normal%20.JPG)


`@part2`
b). MLE of Bernoulli Parameter
![](https://assets.datacamp.com/production/repositories/4490/datasets/7c9ed1d4bef393a078f73543dc6ed42c9727eca6/MLE%20of%20Bernoulli%20Parameter.JPG)


`@part3`
c). MLE of Poisson Parameter 
![](https://assets.datacamp.com/production/repositories/4490/datasets/58fa11a77c16d4183129409efd6c22c9a866d2d0/MLE%20of%20Poisson%20Parameter.JPG)


`@part4`



`@script`
In this slide you can see MLE for few of the common distributions.


---
## Connecting LS to MLE

```yaml
type: "FullSlide"
key: "de5a216e98"
```

`@part1`
![](image-url)![](https://assets.datacamp.com/production/repositories/4490/datasets/c5227c8fac6c508e91e3208f0281051724204c3e/LStoMLE_slide.JPG)


`@script`
I believe by now you would be wondering how MLE is associated with Multiple Linear regression. In the next few slides, we'll see how.

First, We will see the refined equation of Multiple Linear Regression.
where epsilon is Random error for each prediction and we assume it is a normal distribution and independent of X.
 
Our goal is to find a vector of βs that minimize the L2 norm squared of this error.


---
## Connecting LS to MLE

```yaml
type: "FullSlide"
key: "1a6dd72f2a"
```

`@part1`
- LS can defined as ![](https://assets.datacamp.com/production/repositories/4490/datasets/f0bc0138d4f76d36966360a2c115ceecd0db2c51/LStoMLE_slide_2.JPG)
- Likelihood function 
![](https://assets.datacamp.com/production/repositories/4490/datasets/42cdc4d62c1642da4c65c551fd68e17d9b591792/LStoMLE_slide_3.JPG)
- Plugging mean and variance 
![](https://assets.datacamp.com/production/repositories/4490/datasets/dd272a4e9bdfd00b7bfe9c150a16579c47291994/LStoMLE_slide_4.JPG)
which upon solving is equivalent to LS.


`@script`
Now, we can see Least square which we learnt from previous lessons. 

and Likelihood of the data given parameter Betha can be denoted by 2 equation.

And As we assumed the Distribution is normal we will plug the value of mean to be zero and variance sigma square which will help us get the third equation, upon solving which we will find it is same as Least Square. 

Before we move to the next slide, You can try deriving the formula on your own.


---
## Mathematical Derivation

```yaml
type: "FullSlide"
key: "eb762dd688"
```

`@part1`
![](https://assets.datacamp.com/production/repositories/4490/datasets/5a4983e375574534f5f0e0a2af1a4f6a34ade90d/Derivation.JPG)


`@script`
I hope you could derive on your own to see how Least square is same as Maximum Likelihood Estimate. 
If not, you can go through this slide and try it again. I request you to pause the video and take your own time to go through it.


---
## Let's Practice!

```yaml
type: "FinalSlide"
key: "a1e6ec539c"
```

`@script`
Now it's your turn. Let's practice it.

